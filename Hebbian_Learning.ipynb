{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "task:\n",
        "    and, or, not, nand, nor gate\n",
        "    - no function no numpy\n",
        "    - function no numpy\n",
        "    - numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. No Function and No Numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1  X2  y W1  W2  b  \n",
            "-1  -1  -1 0  0\n",
            "-1  1  -1 1  1\n",
            "1  -1  -1 2  0\n",
            "1  1  1 1  1\n",
            "***************************************************************************\n",
            "Trained Weights: w1=2, w2=2, bias=-2\n",
            "X1  X2  W1  W2  b  Y\n",
            "-1  -1  2  2  -6 -1\n",
            "-1  1  2  2  -2 -1\n",
            "1  -1  2  2  -2 -1\n",
            "1  1  2  2  2 1\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1  X2  y W1  W2  b  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}  {x2}  {y} {w1}  {w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "print(\"*****\"*15)\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1  X2  W1  W2  b  Y\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}  {x2}  {w1}  {w2}  {net} {r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y1MMzMvIefP",
        "outputId": "0ba5d4b6-4dda-4423-fd6e-d69864bc36e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1  X2  y W1  W2  b  \n",
            "-1  -1  -1 0  0\n",
            "-1  1  -1 1  1\n",
            "1  -1  -1 2  0\n",
            "1  1  1 1  1\n",
            "Trained Weights: w1=2, w2=2, bias=-2\n",
            "X1  X2  W1  W2  b  Y\n",
            "-1  -1  2  2  -6 -1\n",
            "-1  1  2  2  -2 -1\n",
            "1  -1  2  2  -2 -1\n",
            "1  1  2  2  2 1\n"
          ]
        }
      ],
      "source": [
        "#Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1  X2  y W1  W2  b  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}  {x2}  {y} {w1}  {w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1  X2  W1  W2  b  Y\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}  {x2}  {w1}  {w2}  {net} {r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0D_FK1QJS2a",
        "outputId": "e5ce871b-1abe-457e-d34e-f0d7f8893196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1  X2  y W1  W2  b  \n",
            "-1  -1  -1 0  0\n",
            "-1  1  -1 1  1\n",
            "1  -1  -1 2  0\n",
            "1  1  1 1  1\n",
            "Trained Weights: w1=2, w2=2, bias=-2\n",
            "X1  X2  W1  W2  b  Y\n",
            "-1  -1  2  2  -6 -1\n",
            "-1  1  2  2  -2 -1\n",
            "1  -1  2  2  -2 -1\n",
            "1  1  2  2  2 1\n"
          ]
        }
      ],
      "source": [
        "# Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1  X2  y W1  W2  b  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}  {x2}  {y} {w1}  {w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1  X2  W1  W2  b  Y\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}  {x2}  {w1}  {w2}  {net} {r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehMRaeCBMW5c",
        "outputId": "55701a3a-3051-4591-c811-9c7bd4e8cf2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(2, 3)\n",
            "6\n",
            "[5 7 9]\n",
            "[ 4 10 18]\n",
            "32\n",
            "[[19 22]\n",
            " [43 50]]\n",
            "2.5\n",
            "10\n",
            "4\n",
            "[3 4 5]\n"
          ]
        }
      ],
      "source": [
        "###Importing NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "##Creating Arrays\n",
        "\n",
        "a = np.array([1, 2, 3])  # 1D array\n",
        "b = np.array([[1, 2, 3], [4, 5, 6]])  # 2D array\n",
        "\n",
        "###Shape & Size\n",
        "\n",
        "print(a.shape)  # (3,)\n",
        "print(b.shape)  # (2, 3)\n",
        "print(b.size)   # 6 (total elements)\n",
        "\n",
        "####Generating Arrays\n",
        "\n",
        "zeros = np.zeros((2, 3))  # 2x3 matrix of zeros\n",
        "ones = np.ones((3, 3))    # 3x3 matrix of ones\n",
        "rand = np.random.rand(3, 3)  # 3x3 matrix of random values\n",
        "\n",
        "####Basic Operations\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "\n",
        "print(x + y)  # [5 7 9] (element-wise addition)\n",
        "print(x * y)  # [4 10 18] (element-wise multiplication)\n",
        "print(np.dot(x, y))  # 32 (dot product)\n",
        "\n",
        "###Matrix Multiplication\n",
        "\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "print(np.dot(A, B))  # Matrix multiplication\n",
        "\n",
        "####Mean, Sum, Max\n",
        "\n",
        "arr = np.array([1, 2, 3, 4])\n",
        "print(np.mean(arr))  # 2.5\n",
        "print(np.sum(arr))   # 10\n",
        "print(np.max(arr))   # 4\n",
        "\n",
        "###Conditional Selection\n",
        "\n",
        "arr = np.array([1, 2, 3, 4, 5])\n",
        "print(arr[arr > 2])  # [3 4 5] (elements that are greater than 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djVDwJVHNnjo",
        "outputId": "5961a630-b81e-41fa-e644-b7708b2bb96f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1  X2  y  W1  W2  b\n",
            "-1  -1  -1  0.0  0.0  0\n",
            "-1  1  -1  1.0  1.0  -1\n",
            "1  -1  -1  2.0  0.0  -2\n",
            "1  1  1  1.0  1.0  -3\n",
            "Trained Weights: w1=2.0, w2=2.0, bias=-2\n",
            "X1  X2  W1  W2  b  Y\n",
            "-1  -1  2.0  2.0  -2  -1\n",
            "-1  1  2.0  2.0  -2  -1\n",
            "1  -1  2.0  2.0  -2  -1\n",
            "1  1  2.0  2.0  -2  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "print(f\"X1  X2  y  W1  W2  b\")\n",
        "for x1, x2, y in [\n",
        "    (-1, -1, -1),\n",
        "    (-1,  1, -1),\n",
        "    ( 1, -1, -1),\n",
        "    ( 1,  1,  1)\n",
        "]:\n",
        "    print(f\"{x1}  {x2}  {y}  {w[0]}  {w[1]}  {bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1  X2  W1  W2  b  Y\")\n",
        "for x1, x2, y in [\n",
        "    (-1, -1, -1),\n",
        "    (-1,  1, -1),\n",
        "    ( 1, -1, -1),\n",
        "    ( 1,  1,  1)\n",
        "]:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}  {x2}  {w[0]}  {w[1]}  {bias}  {r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDWxKt4INJlX",
        "outputId": "13646de1-0f9d-4479-ac80-59cb7a66f685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1  X2  y  W1  W2  b\n",
            "-1  -1  -1  0.0  0.0  0\n",
            "-1  1  -1  1.0  1.0  -1\n",
            "1  -1  -1  2.0  0.0  -2\n",
            "1  1  1  1.0  1.0  -3\n",
            "Trained Weights: w1=2.0, w2=2.0, bias=-2\n",
            "X1  X2  W1  W2  b  Y\n",
            "-1  -1  2.0  2.0  -2  -1\n",
            "-1  1  2.0  2.0  -2  -1\n",
            "1  -1  2.0  2.0  -2  -1\n",
            "1  1  2.0  2.0  -2  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1  X2  y  W1  W2  b\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}  {x2}  {y}  {w[0]}  {w[1]}  {bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1  X2  W1  W2  b  Y\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}  {x2}  {w[0]}  {w[1]}  {bias}  {r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1, -1],  # AND(-1, -1) -> -1\n",
        "    [-1,  1, -1],  # AND(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # AND( 1, -1) -> -1\n",
        "    [ 1,  1,  1]   # AND( 1,  1) -> 1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuLrWNcCOrGH",
        "outputId": "73904482-137e-4058-9d8b-e9384009eb35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1  X2  y  W1  W2  b  Y\n",
            "-1  -1  -1  0.1  0.1  -0.1  0.0\n",
            "-1  1  -1  0.19  0.009999999999999995  -0.19  -0.1\n",
            "1  -1  -1  0.091  0.109  -0.28900000000000003  -0.010000000000000009\n",
            "1  1  1  0.1999  0.21789999999999998  -0.18010000000000004  -0.08900000000000002\n",
            "-1  -1  -1  0.24011  0.25811  -0.22031000000000003  -0.5979\n",
            "-1  1  -1  0.319879  0.178341  -0.30007900000000004  -0.20231000000000002\n",
            "1  -1  -1  0.23573310000000003  0.26248689999999997  -0.38422490000000004  -0.15854100000000002\n",
            "1  1  1  0.32433359000000006  0.35108739  -0.29562441  0.11399509999999996\n",
            "-1  -1  -1  0.32722905100000005  0.353982851  -0.298519871  -0.97104539\n",
            "-1  1  -1  0.40005244390000005  0.2811594581  -0.3713432639  -0.27176607100000005\n",
            "1  -1  -1  0.32529747171000006  0.35591443029  -0.44609823609  -0.2524502780999999\n",
            "1  1  1  0.40178610511900004  0.43240306369899995  -0.36960960268099996  0.23511366591000005\n",
            "-1  -1  -1  0.38140622796910006  0.4120231865491  -0.3492297255311  -1.203798771499\n",
            "-1  1  -1  0.4495449512739901  0.34388446324420996  -0.41736844883599  -0.3186127669511001\n",
            "1  -1  -1  0.38071574735461106  0.412713667163589  -0.486197652755369  -0.3117079608062099\n",
            "1  1  1  0.44999257117832797  0.4819904909873059  -0.4169208289316521  0.307231761762831\n",
            "-1  -1  -1  0.4151021820685994  0.4471001018775773  -0.38203043982192353  -1.348903891097286\n",
            "-1  1  -1  0.48009893006730486  0.38210335387887184  -0.447027187820629  -0.3500325200129456\n",
            "1  -1  -1  0.41500209123052445  0.44720019271565226  -0.5121240266574094  -0.349031611632196\n",
            "1  1  1  0.4799942655016477  0.5121923669867755  -0.44713185238628617  0.3500782572887673\n",
            "-1  -1  -1  0.43606241701417675  0.46826051849930456  -0.4032000038988152  -1.4393184848747094\n",
            "-1  1  -1  0.498962226772808  0.4053607087406733  -0.46609981365744646  -0.3710019024136874\n",
            "1  -1  -1  0.43621205633533916  0.46811087917814215  -0.5288499840949152  -0.3724982956253118\n",
            "1  1  1  0.4986647611934826  0.5305635840362856  -0.4663972792367718  0.37547295141856607\n",
            "-1  -1  -1  0.44910219874682855  0.48100102158963154  -0.4168347167901178  -1.49562562446654\n",
            "-1  1  -1  0.5106086093520971  0.419494610984363  -0.4783411273953863  -0.3849358939473148\n",
            "1  -1  -1  0.4493313222548623  0.4807718980815978  -0.539618414492621  -0.3872271290276523\n",
            "1  1  1  0.5102828416704783  0.5417234174972139  -0.47866689507700494  0.39048480584383904\n",
            "-1  -1  -1  0.4572155262460086  0.4886561020727441  -0.4255995796525352  -1.5306731542446972\n",
            "-1  1  -1  0.5177996258634286  0.42807200245532406  -0.48618367926995526  -0.39415900382579966\n",
            "1  -1  -1  0.4574452314496137  0.488426396869139  -0.5465380736837702  -0.3964560558618507\n",
            "1  1  1  0.5175118759861155  0.5484930414056408  -0.48647142914726843  0.39933355463498255\n",
            "-1  -1  -1  0.462264241332213  0.49324540675173834  -0.43122379449336595  -1.5524763465390248\n",
            "-1  1  -1  0.5222399784248289  0.4332696696591224  -0.4911995315859819  -0.4002426290738406\n",
            "1  -1  -1  0.4624629007068565  0.49304674737709486  -0.5509766093039543  -0.40222922282027535\n",
            "1  1  1  0.5220095968288567  0.5525934434990951  -0.491429913181954  0.404533038779997\n",
            "-1  -1  -1  0.46540630147786616  0.49599014814810455  -0.43482661783096344  -1.5660329535099058\n",
            "-1  1  -1  0.5249820243617936  0.43641442526417706  -0.4944023407148909  -0.40424277116072505\n",
            "1  -1  -1  0.4655654985235211  0.4958309511024496  -0.5538188665531635  -0.40583474161727434\n",
            "1  1  1  0.5248077402162403  0.5550731927951689  -0.4945766248604442  0.4075775830728072\n",
            "Trained Weights: w1=0.5248077402162403, w2=0.5550731927951689, bias=-0.4945766248604442\n",
            "X1  X2  W1  W2  b  Y\n",
            "-1  -1  0.5248077402162403  0.5550731927951689  -0.4945766248604442  -1.5744575578718534\n",
            "-1  1  0.5248077402162403  0.5550731927951689  -0.4945766248604442  -0.46431117228151564\n",
            "1  -1  0.5248077402162403  0.5550731927951689  -0.4945766248604442  -0.5248420774393727\n",
            "1  1  0.5248077402162403  0.5550731927951689  -0.4945766248604442  0.585304308150965\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for AND Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    [-1, -1, -1],  # AND(-1, -1) -> -1\n",
        "    [-1,  1, -1],  # AND(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # AND( 1, -1) -> -1\n",
        "    [ 1,  1,  1]   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1  X2  y  W1  W2  b  Y\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}  {x2}  {y}  {w[0]}  {w[1]}  {bias}  {predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1  X2  W1  W2  b  Y\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}  {x2}  {w[0]}  {w[1]}  {bias}  {predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HMG7p5mlrQb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
